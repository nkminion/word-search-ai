{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235befc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using the GPU\n",
      "Found 1 GPUs\n",
      "GPU 0 found: NVIDIA GeForce RTX 5070 Laptop GPU\n",
      "Selected Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Importing packages and checking GPU\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from PIL import Image,ImageDraw,ImageFont\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\tprint(\"PyTorch is using the GPU\")\n",
    "\tGPUCount = torch.cuda.device_count()\n",
    "\tprint(f\"Found {GPUCount} GPUs\")\n",
    "\n",
    "\tfor i in range(GPUCount):\n",
    "\t\tprint(f\"GPU {i} found: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "\tdevice = torch.device(\"cuda:0\")\n",
    "else:\n",
    "\tprint(\"PyTorch is using the CPU\")\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Selected Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e172ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load BuiltIn dataset\n",
    "Transform = transforms.Compose(\n",
    "\t[transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5,),(0.5,))])\n",
    "Transform = transforms.Compose(\n",
    "\t[transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5,),(0.5,))])\n",
    "\n",
    "# TrainSet = torchvision.datasets.EMNIST(root='./data',train=True,download=True,transform=Transform,split=\"letters\",target_transform=lambda y:y-1)\n",
    "# TrainImages = TrainSet.data.numpy()\n",
    "# TrainLabels = TrainSet.targets.numpy() - 1\n",
    "\n",
    "# TestSet = torchvision.datasets.EMNIST(root='./data',train=False,download=True,transform=Transform,split=\"letters\",target_transform=lambda y:y-1)\n",
    "# TestImages = TestSet.data.numpy()\n",
    "# TestLabels = TestSet.targets.numpy() - 1\n",
    "\n",
    "# print(f\"Shape of train data: {TrainImages.shape}\")\n",
    "# print(f\"Shape of test data: {TestImages.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51c2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load font fontsNishant!\n",
      "Could not load font fontsNishant!\n",
      "Could not load font fontsNishant!\n",
      "Shape of custom data: (117390, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#Load Custom dataset\n",
    "\n",
    "FontPath = 'fontsNishant'\n",
    "Images = []\n",
    "Labels = []\n",
    "\n",
    "for Font in os.listdir(FontPath):\n",
    "\tif Font.endswith('ttf'):\n",
    "\t\ttry:\n",
    "\t\t\tfont = ImageFont.truetype(os.path.join(FontPath,Font),24)\n",
    "\t\t\tfor i in range(26):\n",
    "\t\t\t\timg = Image.new('L',(28,28),color=0)\n",
    "\t\t\t\tdraw = ImageDraw.Draw(img)\n",
    "\t\t\t\tdraw.text((8,2),str(chr(ord('A') + i)),font=font,fill=255)\n",
    "\t\t\t\tImages.append(np.array(img))\n",
    "\t\t\t\tLabels.append(i)\n",
    "\t\texcept (OSError,IOError):\n",
    "\t\t\tprint(f\"Could not load font {Font}!\")\n",
    "\n",
    "Images = np.array(Images)\n",
    "Images = Images.astype(\"float32\") / 255.0\n",
    "Images = (Images - 0.5) / 0.5\n",
    "Labels = np.array(Labels)\n",
    "\n",
    "print(f\"Shape of custom data: {Images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "624283cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined data: (117390, 28, 28)\n",
      "Shape of train data: (93912, 28, 28)\n",
      "Shape of test data: (23478, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#Combine datasets and split into new ones\n",
    "\n",
    "# Images = np.concatenate((TrainImages,TestImages,Images),axis=0)\n",
    "# Labels = np.concatenate((TrainLabels,TestLabels,Labels),axis=0)\n",
    "\n",
    "print(f\"Shape of combined data: {Images.shape}\")\n",
    "\n",
    "TrainImages,TestImages,TrainLabels,TestLabels = train_test_split(Images,Labels,test_size=0.2,shuffle=True,stratify=Labels)\n",
    "\n",
    "print(f\"Shape of train data: {TrainImages.shape}\")\n",
    "print(f\"Shape of test data: {TestImages.shape}\")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "\tdef __init__(self,images,labels,transform=None):\n",
    "\t\tself.images = images\n",
    "\t\tself.labels = labels\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.images)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\timage = self.images[index]\n",
    "\t\tlabel = self.labels[index]\n",
    "\n",
    "\t\tif self.transform:\n",
    "\t\t\timage = self.transform(image)\n",
    "\n",
    "\t\treturn image,label\t\n",
    "\t\n",
    "TrainDataset = CustomDataset(TrainImages,TrainLabels,Transform)\n",
    "TestDataset = CustomDataset(TestImages,TestLabels,Transform)\n",
    "\n",
    "TrainLoader = DataLoader(TrainDataset,batch_size=128,shuffle=True)\n",
    "TestLoader = DataLoader(TestDataset,batch_size=128,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3889778c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating EMNIST Model...\n",
      "Model Created\n",
      "Model Summary: \n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             288\n",
      "       BatchNorm2d-2           [-1, 32, 28, 28]              64\n",
      "              ReLU-3           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-4           [-1, 32, 14, 14]               0\n",
      "            Conv2d-5           [-1, 32, 14, 14]           9,216\n",
      "       BatchNorm2d-6           [-1, 32, 14, 14]              64\n",
      "              ReLU-7           [-1, 32, 14, 14]               0\n",
      "            Conv2d-8           [-1, 64, 14, 14]          18,432\n",
      "       BatchNorm2d-9           [-1, 64, 14, 14]             128\n",
      "             ReLU-10           [-1, 64, 14, 14]               0\n",
      "           Conv2d-11           [-1, 64, 14, 14]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 14, 14]             128\n",
      "             ReLU-13           [-1, 64, 14, 14]               0\n",
      "           Conv2d-14           [-1, 64, 14, 14]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 14, 14]             128\n",
      "             ReLU-16           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-17             [-1, 64, 7, 7]               0\n",
      "          Flatten-18                 [-1, 3136]               0\n",
      "           Linear-19                   [-1, 64]         200,768\n",
      "             ReLU-20                   [-1, 64]               0\n",
      "           Linear-21                   [-1, 64]           4,160\n",
      "             ReLU-22                   [-1, 64]               0\n",
      "          Dropout-23                   [-1, 64]               0\n",
      "           Linear-24                   [-1, 26]           1,690\n",
      "================================================================\n",
      "Total params: 308,794\n",
      "Trainable params: 308,794\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.68\n",
      "Params size (MB): 1.18\n",
      "Estimated Total Size (MB): 2.86\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Creating Model\n",
    "\n",
    "#Define architecture\n",
    "class EMNISTModel(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(EMNISTModel,self).__init__()\n",
    "\n",
    "\t\t#Layer1\n",
    "\t\tself.conv1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,padding=1,bias=False)#Size does not change\n",
    "\t\tself.bn1 = nn.BatchNorm2d(num_features=32)\n",
    "\t\tself.relu1 = nn.ReLU()\n",
    "\t\tself.pool1 = nn.MaxPool2d(kernel_size=2)#Size halves into 14x14\n",
    "\n",
    "\t\t#Layer2\n",
    "\t\tself.conv2 = nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,padding=1,bias=False)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(num_features=32)\n",
    "\t\tself.relu2 = nn.ReLU()\n",
    "\n",
    "\t\t#Layer3\n",
    "\t\tself.conv3 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1,bias=False)\n",
    "\t\tself.bn3 = nn.BatchNorm2d(num_features=64)\n",
    "\t\tself.relu3 = nn.ReLU()\n",
    "\n",
    "\t\t#Layer4\n",
    "\t\tself.conv4 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1,bias=False)\n",
    "\t\tself.bn4 = nn.BatchNorm2d(num_features=64)\n",
    "\t\tself.relu4 = nn.ReLU()\n",
    "\n",
    "\t\t#Layer5\n",
    "\t\tself.conv5 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1,bias=False)\n",
    "\t\tself.bn5 = nn.BatchNorm2d(num_features=64)\n",
    "\t\tself.relu5 = nn.ReLU()\n",
    "\t\tself.pool2 = nn.MaxPool2d(kernel_size=2)#Size halves into 7x7\n",
    "\n",
    "\t\t#FlattenLayer\n",
    "\t\tself.flatten = nn.Flatten()\n",
    "\n",
    "\t\t#Layer6\n",
    "\t\tself.fc1 = nn.Linear(in_features=64*7*7,out_features=64)\n",
    "\t\tself.relu6 = nn.ReLU()\n",
    "\n",
    "\t\t#Layer7\n",
    "\t\tself.fc2 = nn.Linear(in_features=64,out_features=64)\n",
    "\t\tself.relu7 = nn.ReLU()\n",
    "\n",
    "\t\t#DropoutLayer\n",
    "\t\tself.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "\t\t#Layer8\n",
    "\t\tself.fc3 = nn.Linear(in_features=64,out_features=26)\n",
    "\n",
    "\tdef forward(self,x):\n",
    "\n",
    "\t\t#Pass through Layer1\n",
    "\t\tx = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "\n",
    "\t\t#Pass through Layer2\n",
    "\t\tx = self.relu2(self.bn2(self.conv2(x)))\n",
    "\n",
    "\t\t#Pass through Layer3\n",
    "\t\tx = self.relu3(self.bn3(self.conv3(x)))\n",
    "\n",
    "\t\t#Pass through Layer4\n",
    "\t\tx = self.relu4(self.bn4(self.conv4(x)))\n",
    "\n",
    "\t\t#Pass through Layer5\n",
    "\t\tx = self.pool2(self.relu5(self.bn5(self.conv5(x))))\n",
    "\n",
    "\t\t#Pass through Layer5\n",
    "\t\tx = self.flatten(x)\n",
    "\n",
    "\t\t#Pass through Layer6\n",
    "\t\tx = self.relu6(self.fc1(x))\n",
    "\n",
    "\t\t#Pass through Layer7\n",
    "\t\tx = self.relu7(self.fc2(x))\n",
    "\n",
    "\t\t#Pass through DropoutLayer\n",
    "\t\tx = self.dropout(x)\n",
    "\n",
    "\t\t#Pass through Layer7\n",
    "\t\tx = self.fc3(x)\n",
    "\n",
    "\t\t#Return Prediction\n",
    "\t\treturn x\n",
    "\n",
    "#Create and print summary\t\n",
    "print(\"Creating EMNIST Model...\")\n",
    "model = EMNISTModel().to(device)\n",
    "print(\"Model Created\")\n",
    "\n",
    "print(\"Model Summary: \")\n",
    "summary(model,input_size=(1,28,28))\n",
    "\n",
    "#Compile model\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360e44df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m \toptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m \t\u001b[38;5;66;03m#Update loss\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \tTrainLoss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#Validation Loop\u001b[39;00m\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training and saving the model\n",
    "\n",
    "print(\"Training Model...\")\n",
    "epochs = 100\n",
    "patience = 5\n",
    "counter = 0\n",
    "MaxAccuracy = float('-inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\t#Train Loop\n",
    "\n",
    "\t#Set model to training mode\n",
    "\tmodel.train()\n",
    "\tTrainLoss = 0.0\n",
    "\n",
    "\tfor i,data in enumerate(TrainLoader,0):\n",
    "\t\tinputs,labels = data[0].to(device),data[1].to(device)\n",
    "\n",
    "\t\t#Zero the parameter gradients\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t#Forward pass\n",
    "\t\toutputs = model(inputs)\n",
    "\n",
    "\t\t#Calculate loss\n",
    "\t\tloss = loss_function(outputs,labels)\n",
    "\n",
    "\t\t#Backward pass\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\t#Update weights\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t#Update loss\n",
    "\t\tTrainLoss += loss.item()\n",
    "\t\n",
    "\t#Validation Loop\n",
    "\tmodel.eval()\n",
    "\tValidationLoss = 0.0\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor data in TestLoader:\n",
    "\t\t\timages,labels = data[0].to(device),data[1].to(device)\n",
    "\t\t\toutputs = model(images)\n",
    "\t\t\tloss = loss_function(outputs,labels)\n",
    "\t\t\tValidationLoss += loss.item()\n",
    "\n",
    "\t\t\t_,predicted = torch.max(outputs.data,1)\n",
    "\t\t\ttotal += labels.size(0)\n",
    "\t\t\tcorrect += (predicted == labels).sum().item()\n",
    "\n",
    "\t\tacc = 100*correct/total\n",
    "\t\tprint(f\"Epoch: {epoch+1}/{epochs} | \"\n",
    "\t\t\tf\"Training Loss: {TrainLoss/len(TrainLoader):.3f} | \"\n",
    "\t\t\tf\"Validation Loss: {ValidationLoss / len(TestLoader):.3f} | \"\n",
    "\t\t\tf\"Accuracy: {acc:.3f}%\")\n",
    "\t\t\n",
    "\t\tif acc > MaxAccuracy:\n",
    "\t\t\tMaxAccuracy = acc\n",
    "\t\t\tcounter = 0\n",
    "\t\t\ttorch.save(model.state_dict(),\"EMNISTModel.pth\")\n",
    "\t\t\tprint(f\"Accuracy has improved at epoch {epoch+1}. Model Saved!\")\n",
    "\t\telse:\n",
    "\t\t\tcounter += 1\n",
    "\t\t\tprint(f\"Accuracy has not improved. Patience:{counter}/{patience}\")\n",
    "\t\t\n",
    "\t\tif counter >= patience:\n",
    "\t\t\tprint(\"Early stopping triggered!\")\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2021dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model with inbuilt dataset...\n",
      "Test Accuracy: 71.754%\n",
      "Test Loss: 0.955\n"
     ]
    }
   ],
   "source": [
    "#Testing Model\n",
    "\n",
    "print(\"Testing model with inbuilt dataset...\")\n",
    "\n",
    "\n",
    "\t\t#Update loss\n",
    "model.load_state_dict(torch.load(\"EMNISTModel.pth\"))\n",
    "\n",
    "TestLoss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfor images, labels in TestLoader:\n",
    "\n",
    "\t\t#Move data to device\n",
    "\t\timages,labels = images.to(device),labels.to(device)\n",
    "\n",
    "\t\t#Forward pass\n",
    "\t\toutputs = model(images)\n",
    "\n",
    "\t\t#Calculate loss\n",
    "\t\tloss = loss_function(outputs,labels)\n",
    "\t\tTestLoss += loss.item()\n",
    "\n",
    "\t\t#Get predicted class\n",
    "\t\t_,predicted = torch.max(outputs.data,1)\n",
    "\n",
    "\t\t#Update total and correct counts\n",
    "\t\ttotal += labels.size(0)\n",
    "\t\tcorrect += (predicted == labels).sum().item()\n",
    "\t\n",
    "#Test Results\n",
    "\n",
    "FinalLoss = TestLoss/len(TestLoader)\n",
    "FinalAcc = 100*correct/total\n",
    "\n",
    "print(f\"Test Accuracy: {FinalAcc:.3f}%\")\n",
    "print(f\"Test Loss: {FinalLoss:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
