{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "235befc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is using the GPU\n",
      "Found 1 GPUs\n",
      "GPU 0 found: NVIDIA GeForce RTX 5070 Laptop GPU\n",
      "Selected Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Importing packages and checking GPU\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader,Dataset,random_split\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from PIL import Image,ImageDraw,ImageFont\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\tprint(\"PyTorch is using the GPU\")\n",
    "\tGPUCount = torch.cuda.device_count()\n",
    "\tprint(f\"Found {GPUCount} GPUs\")\n",
    "\n",
    "\tfor i in range(GPUCount):\n",
    "\t\tprint(f\"GPU {i} found: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "\tdevice = torch.device(\"cuda:0\")\n",
    "else:\n",
    "\tprint(\"PyTorch is using the CPU\")\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Selected Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "624283cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of custom data: 18044\n",
      "Size of train data: 14435\n",
      "Size of test data: 3609\n",
      "Dataset loaded successfully\n"
     ]
    }
   ],
   "source": [
    "#Import datasets and split into new ones\n",
    "\n",
    "Transform = transforms.Compose(\n",
    "\t[\n",
    "\t\ttransforms.Grayscale(num_output_channels=1),\n",
    "\t\ttransforms.ToTensor(),\n",
    "  \t\ttransforms.Normalize((0.5,),(0.5,)),\n",
    "\t\ttransforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0)\n",
    "\t])\n",
    "\n",
    "ImagesDirectory = 'Dataset'\n",
    "\n",
    "Dataset = ImageFolder(root=ImagesDirectory , transform=Transform)\n",
    "\n",
    "print(f\"Size of custom data: {len(Dataset)}\")\n",
    "\n",
    "TrainSize = int(0.8*(len(Dataset)))\n",
    "TestSize = len(Dataset)-TrainSize\n",
    "\n",
    "TrainDataset, TestDataset = random_split(Dataset , [TrainSize, TestSize])\n",
    "\n",
    "print(f\"Size of train data: {TrainSize}\")\n",
    "print(f\"Size of test data: {TestSize}\")\n",
    "\n",
    "TrainLoader = DataLoader(TrainDataset,batch_size=128,shuffle=True)\n",
    "TestLoader = DataLoader(TestDataset,batch_size=128,shuffle=False)\n",
    "\n",
    "print(\"Dataset loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3889778c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating EMNIST Model...\n",
      "Model Created\n",
      "Model Summary: \n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             288\n",
      "       BatchNorm2d-2           [-1, 32, 28, 28]              64\n",
      "              ReLU-3           [-1, 32, 28, 28]               0\n",
      "         MaxPool2d-4           [-1, 32, 14, 14]               0\n",
      "            Conv2d-5           [-1, 32, 14, 14]           9,216\n",
      "       BatchNorm2d-6           [-1, 32, 14, 14]              64\n",
      "              ReLU-7           [-1, 32, 14, 14]               0\n",
      "            Conv2d-8           [-1, 64, 14, 14]          18,432\n",
      "       BatchNorm2d-9           [-1, 64, 14, 14]             128\n",
      "             ReLU-10           [-1, 64, 14, 14]               0\n",
      "           Conv2d-11           [-1, 64, 14, 14]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 14, 14]             128\n",
      "             ReLU-13           [-1, 64, 14, 14]               0\n",
      "           Conv2d-14           [-1, 64, 14, 14]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 14, 14]             128\n",
      "             ReLU-16           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-17             [-1, 64, 7, 7]               0\n",
      "          Flatten-18                 [-1, 3136]               0\n",
      "           Linear-19                   [-1, 64]         200,768\n",
      "             ReLU-20                   [-1, 64]               0\n",
      "           Linear-21                   [-1, 64]           4,160\n",
      "             ReLU-22                   [-1, 64]               0\n",
      "          Dropout-23                   [-1, 64]               0\n",
      "           Linear-24                   [-1, 26]           1,690\n",
      "================================================================\n",
      "Total params: 308,794\n",
      "Trainable params: 308,794\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.68\n",
      "Params size (MB): 1.18\n",
      "Estimated Total Size (MB): 2.86\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Creating Model\n",
    "\n",
    "#Define architecture\n",
    "class EMNISTModel(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(EMNISTModel,self).__init__()\n",
    "\n",
    "\t\t#Layer1\n",
    "\t\tself.conv1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,padding=1,bias=False)#Size does not change\n",
    "\t\tself.bn1 = nn.BatchNorm2d(num_features=32)\n",
    "\t\tself.relu1 = nn.ReLU()\n",
    "\t\tself.pool1 = nn.MaxPool2d(kernel_size=2)#Size halves into 14x14\n",
    "\n",
    "\t\t#Layer2\n",
    "\t\tself.conv2 = nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,padding=1,bias=False)\n",
    "\t\tself.bn2 = nn.BatchNorm2d(num_features=32)\n",
    "\t\tself.relu2 = nn.ReLU()\n",
    "\n",
    "\t\t#Layer3\n",
    "\t\tself.conv3 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1,bias=False)\n",
    "\t\tself.bn3 = nn.BatchNorm2d(num_features=64)\n",
    "\t\tself.relu3 = nn.ReLU()\n",
    "\n",
    "\t\t#Layer4\n",
    "\t\tself.conv4 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1,bias=False)\n",
    "\t\tself.bn4 = nn.BatchNorm2d(num_features=64)\n",
    "\t\tself.relu4 = nn.ReLU()\n",
    "\n",
    "\t\t#Layer5\n",
    "\t\tself.conv5 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1,bias=False)\n",
    "\t\tself.bn5 = nn.BatchNorm2d(num_features=64)\n",
    "\t\tself.relu5 = nn.ReLU()\n",
    "\t\tself.pool2 = nn.MaxPool2d(kernel_size=2)#Size halves into 7x7\n",
    "\n",
    "\t\t#FlattenLayer\n",
    "\t\tself.flatten = nn.Flatten()\n",
    "\n",
    "\t\t#Layer6\n",
    "\t\tself.fc1 = nn.Linear(in_features=64*7*7,out_features=64)\n",
    "\t\tself.relu6 = nn.ReLU()\n",
    "\n",
    "\t\t#Layer7\n",
    "\t\tself.fc2 = nn.Linear(in_features=64,out_features=64)\n",
    "\t\tself.relu7 = nn.ReLU()\n",
    "\n",
    "\t\t#DropoutLayer\n",
    "\t\tself.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "\t\t#Layer8\n",
    "\t\tself.fc3 = nn.Linear(in_features=64,out_features=26)\n",
    "\n",
    "\tdef forward(self,x):\n",
    "\n",
    "\t\t#Pass through Layer1\n",
    "\t\tx = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "\n",
    "\t\t#Pass through Layer2\n",
    "\t\tx = self.relu2(self.bn2(self.conv2(x)))\n",
    "\n",
    "\t\t#Pass through Layer3\n",
    "\t\tx = self.relu3(self.bn3(self.conv3(x)))\n",
    "\n",
    "\t\t#Pass through Layer4\n",
    "\t\tx = self.relu4(self.bn4(self.conv4(x)))\n",
    "\n",
    "\t\t#Pass through Layer5\n",
    "\t\tx = self.pool2(self.relu5(self.bn5(self.conv5(x))))\n",
    "\n",
    "\t\t#Pass through Layer5\n",
    "\t\tx = self.flatten(x)\n",
    "\n",
    "\t\t#Pass through Layer6\n",
    "\t\tx = self.relu6(self.fc1(x))\n",
    "\n",
    "\t\t#Pass through Layer7\n",
    "\t\tx = self.relu7(self.fc2(x))\n",
    "\n",
    "\t\t#Pass through DropoutLayer\n",
    "\t\tx = self.dropout(x)\n",
    "\n",
    "\t\t#Pass through Layer7\n",
    "\t\tx = self.fc3(x)\n",
    "\n",
    "\t\t#Return Prediction\n",
    "\t\treturn x\n",
    "\n",
    "#Create and print summary\t\n",
    "print(\"Creating EMNIST Model...\")\n",
    "model = EMNISTModel().to(device)\n",
    "print(\"Model Created\")\n",
    "\n",
    "print(\"Model Summary: \")\n",
    "summary(model,input_size=(1,28,28))\n",
    "\n",
    "#Compile model\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "360e44df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model...\n",
      "Epoch: 1/100 | Training Loss: 1.089 | Validation Loss: 0.163 | Accuracy: 95.456%\n",
      "Validation Loss has improved at epoch 1. Model Saved!\n",
      "Epoch: 2/100 | Training Loss: 0.173 | Validation Loss: 0.127 | Accuracy: 95.871%\n",
      "Validation Loss has improved at epoch 2. Model Saved!\n",
      "Epoch: 3/100 | Training Loss: 0.124 | Validation Loss: 0.086 | Accuracy: 97.506%\n",
      "Validation Loss has improved at epoch 3. Model Saved!\n",
      "Epoch: 4/100 | Training Loss: 0.091 | Validation Loss: 0.070 | Accuracy: 97.451%\n",
      "Validation Loss has improved at epoch 4. Model Saved!\n",
      "Epoch: 5/100 | Training Loss: 0.082 | Validation Loss: 0.062 | Accuracy: 98.088%\n",
      "Validation Loss has improved at epoch 5. Model Saved!\n",
      "Epoch: 6/100 | Training Loss: 0.081 | Validation Loss: 0.063 | Accuracy: 97.922%\n",
      "Validation Loss has not improved. Patience:1/5\n",
      "Epoch: 7/100 | Training Loss: 0.076 | Validation Loss: 0.042 | Accuracy: 98.421%\n",
      "Validation Loss has improved at epoch 7. Model Saved!\n",
      "Epoch: 8/100 | Training Loss: 0.061 | Validation Loss: 0.050 | Accuracy: 98.337%\n",
      "Validation Loss has not improved. Patience:1/5\n",
      "Epoch: 9/100 | Training Loss: 0.071 | Validation Loss: 0.055 | Accuracy: 98.088%\n",
      "Validation Loss has not improved. Patience:2/5\n",
      "Epoch: 10/100 | Training Loss: 0.062 | Validation Loss: 0.054 | Accuracy: 98.116%\n",
      "Validation Loss has not improved. Patience:3/5\n",
      "Epoch: 11/100 | Training Loss: 0.064 | Validation Loss: 0.048 | Accuracy: 98.171%\n",
      "Validation Loss has not improved. Patience:4/5\n",
      "Epoch: 12/100 | Training Loss: 0.060 | Validation Loss: 0.036 | Accuracy: 98.753%\n",
      "Validation Loss has improved at epoch 12. Model Saved!\n",
      "Epoch: 13/100 | Training Loss: 0.059 | Validation Loss: 0.064 | Accuracy: 98.171%\n",
      "Validation Loss has not improved. Patience:1/5\n",
      "Epoch: 14/100 | Training Loss: 0.050 | Validation Loss: 0.055 | Accuracy: 97.894%\n",
      "Validation Loss has not improved. Patience:2/5\n",
      "Epoch: 15/100 | Training Loss: 0.045 | Validation Loss: 0.038 | Accuracy: 98.531%\n",
      "Validation Loss has not improved. Patience:3/5\n",
      "Epoch: 16/100 | Training Loss: 0.053 | Validation Loss: 0.049 | Accuracy: 98.448%\n",
      "Validation Loss has not improved. Patience:4/5\n",
      "Epoch: 17/100 | Training Loss: 0.047 | Validation Loss: 0.040 | Accuracy: 98.698%\n",
      "Validation Loss has not improved. Patience:5/5\n",
      "Early stopping triggered!\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "#Training and saving the model\n",
    "\n",
    "print(\"Training Model...\")\n",
    "epochs = 100\n",
    "patience = 5\n",
    "counter = 0\n",
    "MinValLoss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\t#Train Loop\n",
    "\n",
    "\t#Set model to training mode\n",
    "\tmodel.train()\n",
    "\tTrainLoss = 0.0\n",
    "\n",
    "\tfor i,data in enumerate(TrainLoader,0):\n",
    "\t\tinputs,labels = data[0].to(device),data[1].to(device)\n",
    "\n",
    "\t\t#Zero the parameter gradients\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t#Forward pass\n",
    "\t\toutputs = model(inputs)\n",
    "\n",
    "\t\t#Calculate loss\n",
    "\t\tloss = loss_function(outputs,labels)\n",
    "\n",
    "\t\t#Backward pass\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\t#Update weights\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t#Update loss\n",
    "\t\tTrainLoss += loss.item()\n",
    "\t\n",
    "\t#Validation Loop\n",
    "\tmodel.eval()\n",
    "\tValidationLoss = 0.0\n",
    "\tcorrect = 0\n",
    "\ttotal = 0\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor data in TestLoader:\n",
    "\t\t\timages,labels = data[0].to(device),data[1].to(device)\n",
    "\t\t\toutputs = model(images)\n",
    "\t\t\tloss = loss_function(outputs,labels)\n",
    "\t\t\tValidationLoss += loss.item()\n",
    "\n",
    "\t\t\t_,predicted = torch.max(outputs.data,1)\n",
    "\t\t\ttotal += labels.size(0)\n",
    "\t\t\tcorrect += (predicted == labels).sum().item()\n",
    "\n",
    "\t\tacc = 100*correct/total\n",
    "\t\tprint(f\"Epoch: {epoch+1}/{epochs} | \"\n",
    "\t\t\tf\"Training Loss: {TrainLoss/len(TrainLoader):.3f} | \"\n",
    "\t\t\tf\"Validation Loss: {ValidationLoss / len(TestLoader):.3f} | \"\n",
    "\t\t\tf\"Accuracy: {acc:.3f}%\")\n",
    "\t\t\n",
    "\t\tif ValidationLoss < MinValLoss:\n",
    "\t\t\tMinValLoss = ValidationLoss\n",
    "\t\t\tcounter = 0\n",
    "\t\t\ttorch.save(model.state_dict(),\"EMNISTModel.pth\")\n",
    "\t\t\tprint(f\"Validation Loss has improved at epoch {epoch+1}. Model Saved!\")\n",
    "\t\telse:\n",
    "\t\t\tcounter += 1\n",
    "\t\t\tprint(f\"Validation Loss has not improved. Patience:{counter}/{patience}\")\n",
    "\t\t\n",
    "\t\tif counter >= patience:\n",
    "\t\t\tprint(\"Early stopping triggered!\")\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b2021dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model with inbuilt dataset...\n",
      "Test Accuracy: 98.559%\n",
      "Test Loss: 0.042\n"
     ]
    }
   ],
   "source": [
    "#Testing Model\n",
    "\n",
    "print(\"Testing model with inbuilt dataset...\")\n",
    "\n",
    "\n",
    "\t\t#Update loss\n",
    "model.load_state_dict(torch.load(\"EMNISTModel.pth\"))\n",
    "\n",
    "TestLoss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfor images, labels in TestLoader:\n",
    "\n",
    "\t\t#Move data to device\n",
    "\t\timages,labels = images.to(device),labels.to(device)\n",
    "\n",
    "\t\t#Forward pass\n",
    "\t\toutputs = model(images)\n",
    "\n",
    "\t\t#Calculate loss\n",
    "\t\tloss = loss_function(outputs,labels)\n",
    "\t\tTestLoss += loss.item()\n",
    "\n",
    "\t\t#Get predicted class\n",
    "\t\t_,predicted = torch.max(outputs.data,1)\n",
    "\n",
    "\t\t#Update total and correct counts\n",
    "\t\ttotal += labels.size(0)\n",
    "\t\tcorrect += (predicted == labels).sum().item()\n",
    "\t\n",
    "#Test Results\n",
    "\n",
    "FinalLoss = TestLoss/len(TestLoader)\n",
    "FinalAcc = 100*correct/total\n",
    "\n",
    "print(f\"Test Accuracy: {FinalAcc:.3f}%\")\n",
    "print(f\"Test Loss: {FinalLoss:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
